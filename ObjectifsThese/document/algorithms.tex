\section{Graphical Lasso for Gaussian mixtures}

The EM algorithm experiences severe performance degradation in high-dimensional setting. A technique to avoid this degradation is by regularizing the parameters of the model. The following algorithm is inspired by the Graphical lasso \citep{glasso07} which penalize the components of the precision matrix of a Gaussian graphical model.\\

We consider $\bx_1,\bx_2,\dots,\bx_N$ a sample of $N$ points drawn from a p-dimensional Gaussian Mixture distribution. We note $\btheta=(\theta_1,\dots,\theta_K)$ with $\theta_j=(\pi_j, \bmu_j, \bSigma_j)$. The penalized log-likelihood is
\begin{equation}\label{pen-log-likelihood}
\ell_n^{pen}(\btheta)=\sum_{i=1}^{N}\log{p_{\btheta}(\bx_i)}+pen(\btheta)=
\sum_{i=1}^{N}\log\bigg\{{\sum_{k=1}^K\pi_k\varphi_{(\bmu_{k},\bSigma_{k})}(\bx_i)}\bigg\} +pen(\btheta) .
\end{equation}
In this problem, we suppose that within a cluster $k$, most couples of features $(Y_i,Y_j)$ are independent given the other features $Y_l \text{ with } i,j,l\in[p], l\neq\{i,j \}$. This property is evaluated by the sparsity of $\bOmega_k=\bSigma_k^{-1}$. 
Therefore, we will chose an $l_1$ regularization $pen(\theta_k)=\lambda_k||\bOmega_k||_{1,1}$ with $\lambda_k\in\RR^+$.\\

The penalization of log-likelihood concerns only the precision matrix $\bOmega_k$. Therefore, regarding the other parameters $(\pi_k, \bmu_k)$ the problem is the same as EM and we can use the same iteration technique as in lemma \ref{lemma1} to maximize the following cost function
\begin{equation}
\label{cost_fun_pen}
F^{pen}(\btheta,\bTau)  = \sum_{k=1}^K \sum_{i=1}^{N} \Big\{\tau_{i,k}\log\varphi_{\bmu_{k},\bSigma_{k}}(\bx_i)+\tau_{i,k}
    \log(\pi_k/\tau_{i,k})\Big\}-\lambda_k||\bOmega_k||_{1,1}.
\end{equation}

Then, the following two optimization problems are really close to EM
\begin{align}
\hat\btheta(\bTau)&\in \arg\max_{\btheta} F(\btheta,\bTau),\qquad \hat\bTau(\btheta)\in \arg\max_{\bTau} F(\btheta,\bTau)
\end{align}
and has explicit solutions regarding the variables $(\pi,\bmu)$ and $\bTau$ given by
\begin{align}
\label{em-sols}
\hat\pi_k     &= \frac1n\sum_{i=1}^n \tau_{i,k},\qquad\hat\bmu_k = \frac1{n\hat\pi_k}\sum_{i=1}^n \tau_{i,k}\bx_i ,\qquad \forall k\in[K],\\
\label{em-sols-tau}
\hat\tau_{i,k}&= \frac{\pi_k\varphi_{\bmu_k,\bSigma_k}(\bx_i)}{\sum_{k'\in[K]}\pi_{k'}\varphi_{\bmu_{k'},\bSigma_{k'}}(\bx_i)},\qquad\forall k\in[K],\ \forall i\in[n].
\end{align}

However, due to the penality $\lambda_k||\bOmega_k||_{1,1}$, the estimation of $\bOmega_k$ is not straightforward.

We introduce the weighted empirical covariance matrix:
\begin{equation}
\bS_{N,k} = \frac{\sum_{n=1}^N\tau_{n,k}(\bx_n-\hat\bmu_k)(\bx_n-\hat\bmu_k)^\top}{N\sum_{n=1}^N\tau_{n,k}}
\end{equation}
For a given $k\in[K]$, considering that $\bOmega_k=\bSigma_k^{-1}$ and the opposite minimization problem regarding $\bOmega_k$, the  equation \ref{pen-log-likelihood} can be rewritten as:
\begin{equation}
    \ell_n^{pen}(\btheta)  = -\frac{1}{2}\log| \bOmega |+\frac{1}{2} tr(\bS_{N,k}\bOmega)+\lambda_k||\bOmega||_{1,1}
\end{equation}
and the minimization problem is:
\begin{equation}
    \bOmega_k \in \argmin_{ \bOmega\succeq 0} \Big\{ -\frac{1}{2}\log| \bOmega |-\frac{1}{2} tr(\bS_{N,k}\bOmega)+\lambda_k||\bOmega||_{1,1}\Big\}
\end{equation}

Our algorithm solves a graphical lasso problem within each cluster. We use a block coordinate ascent algorithm \citep{mazum_lasso} to solve this convex problem as in the Graphical Lasso implementation in R \url{http://statweb.stanford.edu/~tibs/glasso/} \\


The alternative maximization procedure is summarized in the following algorithm.


\begin{figure}[H]
\begin{center}
\mybox{
\begin{minipage}{0.85\linewidth}
\begin{algorithmic}%[1]\tt
%\SetLine%\SetAlgoLined
\small
\STATE {\bfseries Input:} data vectors $\bx_1,\ldots,\bx_n\in\RR^p$ and the number of clusters $K$
\STATE {\bfseries Output:} parameter estimate $\hat\btheta = \{\hat\bmu_k,\hat\bOmega_k,\hat\pi_k\}_{k\in[K]}$
\STATE {\tt 1: Initialize $t=0$, $\btheta=\btheta^0$.}
\STATE {\tt 2: {\bf Repeat}}
\STATE \qquad {\tt 3: Update the parameter $\bTau$:}
\begin{align*}
\tau_{i,k}^{t}  &= \frac{\pi_k^{t}\varphi_{\bmu_k^{t},\bOmega_k^{t}}(\bx_i)}{\sum_{k'\in[K]}\pi^{t}_{k'}\varphi_{\bmu^{t}_{k'},\bOmega^{t}_{k'}}(\bx_i)}.
\end{align*}
\STATE \qquad{\tt 4: Update the parameter $\btheta$:}
\begin{align*}
\pi_k^{t+1}     &= \frac1n\sum_{i=1}^n \tau_{i,k}^t,\qquad \\
\bmu_k^{t+1}    &= \frac1{n\pi_k^{t+1}}\sum_{i=1}^n \tau_{i,k}^t\bx_i\\
\bS_{N,k}         &= \frac{\sum_{n=1}^N\tau_{n,k}^{t+1}(\bx_n-\hat\bmu_k^{t+1})(\bx_n-\hat\bmu_k^{t+1})^\top}{N\sum_{n=1}^N\tau_{n,k}^{t+1}}\\
\bOmega_k^{t+1} & \in \argmin_{ \bOmega\succeq 0} \Big\{ -\frac{1}{2}\log| \bOmega |+\frac{1}{2} tr(\bS_{N,k}\bOmega)+\lambda_k||\bOmega||_{1,1}\Big\}
\end{align*}
\STATE \qquad {\tt 5: increment $t$: $t=t+1$}.
\STATE {\tt 6: {\bf Until} stopping rule.}
\STATE {\tt 7: {\bf Return} $\btheta^{t}$}.
\end{algorithmic}
\end{minipage}}
   \caption{Graphical Lasso algorithm for Gaussian mixtures}
   \label{algo:PEM}
\end{center}
\end{figure}
