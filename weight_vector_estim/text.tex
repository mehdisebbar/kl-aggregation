\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsopn}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{empheq}

%\usepackage{psfrag}
\usepackage{float}
\usepackage{amsfonts}
%\usepackage{epstopdf}
\usepackage{dsfont}
%\usepackage{xcolor,fancybox}
\usepackage{url}
\usepackage{pifont}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
% Need it for \caption*
\usepackage{xspace}
% Fix macro spacing bug

\usepackage{tcolorbox}
\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}

\newtcbox{\mybox}{nobeforeafter,colframe=mycolor,colback=mycolor!10!white,boxrule=0.5pt,arc=4pt,
  boxsep=0pt,left=6pt,right=6pt,top=6pt,bottom=6pt,tcbox raise base}

\usepackage{tikz}
\usetikzlibrary{fit,positioning}
\usetikzlibrary{arrows}


%\usepackage{showkeys}

\usepackage{natbib}
\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage{hyperref}
\hypersetup{
  colorlinks = true,
  urlcolor = blue,
  linkcolor = blue,
  citecolor = blue,
}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{geometry}
\geometry{a4paper, left=30mm, right=30mm, top=30mm, bottom=30mm, nohead}




%\setlength{\arraycolsep}{2pt}
%\setlength{\parskip}{.04in}
%\setlength{\footskip}{30pt}

\let\bb\mathbb       % BlackBoardBold (double letters)


  \def\AA{{\bb A}}\def\CC{{\bb C}}\def\DD{{\bb D}}\def\EE{{\bb E}}
  \def\GG{{\bb G}}\def\HH{{\bb H}}\def\KK{{\bb K}}\def\LL{{\bb L}}
  \def\MM{{\bb M}}\def\QQ{{\bb Q}}\def\TT{{\bb T}}\def\YY{{\bb Y}}
  \def\PP{{\bb P}}\def\II{{\bb I}}\def\WW{{\bb W}}\def\XX{{\bb X}}
  \def\VV{{\bb V}}\def\SS{{\bb S}}\def\BB{{\bb B}}\def\NN{{\bb N}}
  \def\RR{{\bb R}}\def\ZZ{{\bb Z}}\def\FF{{\bb F}}\def\DD{{\bb D}}
  \def\OO{{\bb O}}\def\JJ{{\bb J}}\def\UU{{\bb U}}

\def\cH{\mathcal H}
\def\cY{\mathcal Y}
\def\cX{\mathcal X}
\def\cA{\mathcal A}
\def\mC{\mathcal C}
\def\Ex{\mathbf E}

\def\bb{\mathbb}
%end of header
\def\hat{\widehat}
\def\bfX{\mathbf X}
\def\bfB{\mathbf B}
\def\bfA{\mathbf A}
\def\bSigma{\boldsymbol\Sigma}
\def\bsigma{\boldsymbol\sigma}

\def\bOmega{\boldsymbol\Omega}
\def\bmu{\boldsymbol\mu}
\def\bnu{\boldsymbol\nu}
\def\btau{\boldsymbol\tau}
\def\bTau{\boldsymbol{\mathcal T}}
\def\btheta{{\boldsymbol\theta}}
\def\bTheta{{\boldsymbol\Theta}}
\def\bPi{\boldsymbol\Pi}
\def\bX{\boldsymbol X\!}
\def\bx{\boldsymbol x}
\def\bS{\boldsymbol S}
\def\bbeta{\boldsymbol \beta}
\def\bY{\boldsymbol Y}
\def\bxi{\boldsymbol \xi}
\def\bpi{\boldsymbol \pi}
\def\ci{\perp\!\!\!\perp}
\def\bZ{\boldsymbol Z}
\def\11{\mathds 1}
\def\b1{\mathbf 1}
\def\Pb{\mathbf P}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand*{\pd}[3][]{\ensuremath{\frac{d^{#1} #2}{d#3^{#1}}}}


\parindent=0pt
\parskip=5pt
\renewcommand{\baselinestretch}{1.08}


\newtheorem{rem}{Remark}
\newtheorem{defi}{Definition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{pro}{Proposition}
\newtheorem{theo}{Theorem}
\newtheorem{fact}{Fact}
\newtheorem{hyp}{Hypothesis} 

% Number lines
\usepackage{lineno,xcolor}
\linenumbers
\setlength\linenumbersep{5pt}
\renewcommand\linenumberfont{\normalfont\tiny\sffamily\color{gray}}

\begin{document}

Let consider a general mixture model. We observe $N$ random variables
$x_1, x_2,...,x_N$ which are independently and identically distributed with $x_i\sim f_{\bpi}(x_i)$ where $f_{\bpi}$ is given by:
\begin{equation}
f_{\bpi}(x)=\sum_{j=1}^K\bpi_jf_j(x)
\end{equation}
Let suppose that each component density $f_i$ is known, but not necessarily Gaussian. We will focus on the estimation of the weights vector $\bpi\in\RR^K$ and assume that this vector is sparse. We will focus our study on the performance of the Maximum Likelihood Estimator (et excess risk ?). From a rewriting of the loglikelihood we can define $\Phi_N(\bpi)$ as following:
\begin{equation}
  \Phi_N(\bpi) = -\frac{1}{N}\sum_{i=1}^N\log f_{\bpi}(x_i)
\end{equation}
We can rewrite the minimization problem:
\begin{equation}
\hat\bpi \in \argmin_{\bpi\in\Pi}\big\{\Phi_N(\bpi)\big\},\quad \Pi = \big\{\bpi\in [0,1]^K: \sum_{j=1}^K\pi_j=1\big\}
\end{equation}
For theoretical objectives, we will make the following asumption:
\begin{hyp}
\label{hyp_obs_likely}
All realizations are not probably unlikely to be observed. Therefore $\exists m>0 \text{ such that } f_{\bpi}(x)\geq m$ for all $x\in\{x_1,\dots,x_N\}$.
\end{hyp}
Let denote $\displaystyle M= \max_{x\in \{x_1,\dots,x_N\}, j\in[K]}\{f_j(x)\}$, since $\bpi^T\b1= 1$ then $f_{\bpi}\leq M$. Therefore, $\forall \bpi \in \Pi, f_{\bpi}({x_1,\dots,x_N}) \in [m,M]$. We have the following lemma:
\begin{lem}
Under hypothesis \ref{hyp_obs_likely}, $\Phi_N$ is Lipschitz-smooth and strongly convex.
\begin{proof}
For each $ i\in[K], g_{x_i}(\bpi)= f_{\bpi}(x_i)$ is a linear function defined on the convex compact set $\Pi$ and it's image is the interval $[m,M]$ where $m>0$.  We will prove that $-\log$ is strongly convex on $[m,M]$.
\begin{equation}
\forall x \in [m,M], \frac{1}{M^2}\leq\pd[2]{(-\log)}{x}(x)= \frac{1}{x^2}\leq \frac{1}{m^2}.
\end{equation}
The first inequality proves the $1/M^2$-strong convexity of $-\log$, the second proves that it is $1/m^2$-Lipschitz smooth. The sum of strongly convex functions is strongly convex. Therefore, $\Phi_N$ is strongly convex.
\end{proof}
\end{lem}
With these nice property under assumption \ref{hyp_obs_likely}, the minimization problem can be rewritten as follows:
\begin{equation}
\label{min_prob}
\hat\bpi \in \argmin_{\bpi\in\Pi}\big\{\Phi_N(\bpi)\big\},\quad 
\Pi = \big\{\bpi\in [0,1]^K: \bpi^T\b1=1, 
\forall i\in [N],\sum_{j=1}^K\pi_j f_j(x_i)\geq m
\big\}
\end{equation}
In this work, we will study different loss function: $||\hat\bpi-\bpi^*||_1$, $||\hat\bpi-\bpi^*||_2$ and some $dist(f_{\hat\bpi},f_{\bpi^*})$ (donner un example). It turns out that this problem is close to the regression with random design in the context of transductive learning \cite{bellec2016} since we do not observe the true cluster labels in our problem. We can consider $\Phi_N$ as a function of two random variable $X_i$ and $\bpi$:
\begin{equation}
  \Phi_N(\bpi) = \frac{1}{N}\sum_{i=1}^N\varphi(x_i,\bpi)
\end{equation}
In this setting $\varphi(.,.)$ (in our problem it is $-log(f_.(.))$) is strongly convex and Lipschitz smooth. We will recall some interesting results for our work on regression with random design.\\

Let consider the following trace regression model :
\begin{equation}
Y_i = tr(X_i^T\bfB^*) + \xi_i \quad i=1,\dots,N
\end{equation}
with $B^*\in\RR^{pxq}$ and let assume that $rank(B^*)$ is small. Let denote $\bsigma=[\sigma_1,\dots,\sigma_p]$ the singular values of $B^*$. The rank of this matrix is given by $||\sigma||_0$. Unfortunately, the $L_0$ norm is not convex, we tackle this problem by considering the convex $L_1$ norm $||\bsigma||_1$. Assume the constraint $\bsigma^T\b1=1$, then according to \cite{Koltchinskii2016} (quel theoreme ?) an empirical risk minimization method or a Maximum Likelihood Estimator with this constraint leads to a sparse estimator $\hat \bfB$.\\

Therefore, it might be interesting to compare this result with our problem \ref{min_prob}

\section{Error Bound}
Using the strong convexity property, we have:
\begin{equation}
    \big(\nabla \Phi_n(\pi^*)-\nabla \Phi_n(\hat\pi)\big)^T(\pi^*-\hat\pi)\geq \frac{1}{M^2}||\pi^*-\hat\pi||^2
\end{equation}
By definition of the estimator $\hat\pi$ we have $\nabla \Phi_n(\hat\pi)=0$ therefore, we develop $\nabla \Phi_n(\pi^*)^T(\pi^*-\hat\pi)$; for $l \in [K]$:
\begin{equation}
    [\nabla \Phi_n(\pi^*)]_l=-\frac{1}{N}\sum_{i=1}^N\frac{f_l(x_i)}{\sum_{j=1}^K \pi_j^*f_j(x_i)}
\end{equation}
Therefore:

\begin{align}
    \nabla \Phi_n(\pi^*)^T(\pi^*-\hat\pi)&=-\frac{1}{N}\sum_{l=1}^K\sum_{i=1}^N\frac{f_l(x_i)(\pi_l^*-\hat\pi_l)}{\sum_{j=1}^K\pi_j^*f_j(x_i)}\\
    &=-\frac{1}{N}\sum_{i=1}^N\Big(\frac{\sum_{l=1}^K\pi_l^*f_l(x_i)}{\sum_{j=1}^K\pi_j^*f_j(x_i)}-\frac{\sum_{l=1}^K\hat\pi_l f_l(x_i)}{\sum_{j=1}^K\pi_j^*f_j(x_i)}\Big)\\
    &=\frac{1}{N}\sum_{i=1}^N\Big( \frac{f_{\hat\pi}(x_i)}{f_{\pi^*}(x_i)} -1 \Big) = \frac{1}{N}\sum_{i=1}^N Z_i
\end{align}
The idea is to use the Bernstein inequality, $Z_1,...,Z_N$ are independent real-valued random variables, we need to prove that there exist a constant b such that $\EE[Z_i^2]\leq \infty$ and $|Z_i-\EE{Z_i}|\leq b$.
\begin{equation}
    \EE[Z_i^2]=\EE\Big[\frac{f_{\hat\pi^2}}{f_{\pi^*}^2}-2\frac{f_{\hat\pi}}{f_{\pi^*}}+1\Big]
    =\EE\Big[\frac{f_{\hat\pi^2}}{f_{\pi^*}^2}\Big]-1
\end{equation}
\textbf{Note:} On l'assume pour l'instant, c'est surement le cas si le support de $\hat\pi$ est inclu dans celui de $\pi^*$, en effet, soit $\hat S$ le support de $\hat \pi$ et $S^*$ le support de $\pi^*$, alors, pour tout $x\in \RR^p$ on note $q=\argmax_{l\in \hat S}f_l(x)$ et donc 
\begin{equation}
    \frac{\sum_{l\in\hat S} \hat\pi_l f_l(x)}{\sum_{l\in S^*} \pi_l^* f_l(x)}\leq \frac{f_q(x)}{\pi_q^* f_q(x)}=\frac{1}{\pi_q^*}
\end{equation}
donc 
\begin{equation}
    \EE\Big[\frac{f_{\hat\pi}^2}{f_{\pi^*}^2}\Big]=\int_{\RR^p}\frac{f_{\hat\pi}(x)^2}{f_{\pi^*}(x)^2}f_{\pi^*}(x)dx\leq \max_{l \in \hat S}(\pi_l^*})^{-2}
\end{equation}
il faut donc etudier le support de l'estimateur

We can use now the Bernstein inequality (Ã  completer):
\begin{equation}
    |\overline Z_N - \EE[\overline Z_N]|\leq \sigma_N \Big(\frac{2 \log(2/\delta)}{N}\Big)^{1/2}\Big[ 1+\frac{b}{6N\sigma_N}\Big(\frac{2 \log(2/\delta)}{N}\Big)^{1/2}   \Big]
\end{equation}
Since $\EE[Z_i]=0$, we have:
\begin{equation}
    ||\pi^*-\hat\pi||^2 \leq M^2 \sigma_N \Big(\frac{2 \log(2/\delta)}{N}\Big)^{1/2}\Big[ 1+\frac{b}{6N\sigma_N}\Big(\frac{2 \log(2/\delta)}{N}\Big)^{1/2}   \Big]
\end{equation}

\bibliographystyle{plain}
\bibliography{ref}

\end{document}








