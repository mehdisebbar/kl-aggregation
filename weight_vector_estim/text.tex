\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsopn}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{empheq}

%\usepackage{psfrag}
\usepackage{float}
\usepackage{amsfonts}
%\usepackage{epstopdf}
\usepackage{dsfont}
%\usepackage{xcolor,fancybox}
\usepackage{url}
\usepackage{pifont}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
% Need it for \caption*
\usepackage{xspace}
% Fix macro spacing bug

\usepackage{tcolorbox}
\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}

\newtcbox{\mybox}{nobeforeafter,colframe=mycolor,colback=mycolor!10!white,boxrule=0.5pt,arc=4pt,
  boxsep=0pt,left=6pt,right=6pt,top=6pt,bottom=6pt,tcbox raise base}

\usepackage{tikz}
\usetikzlibrary{fit,positioning}
\usetikzlibrary{arrows}


%\usepackage{showkeys}

\usepackage{natbib}
\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage{hyperref}
\hypersetup{
  colorlinks = true,
  urlcolor = blue,
  linkcolor = blue,
  citecolor = blue,
}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{geometry}
\geometry{a4paper, left=30mm, right=30mm, top=30mm, bottom=30mm, nohead}




%\setlength{\arraycolsep}{2pt}
%\setlength{\parskip}{.04in}
%\setlength{\footskip}{30pt}

\let\bb\mathbb       % BlackBoardBold (double letters)


  \def\AA{{\bb A}}\def\CC{{\bb C}}\def\DD{{\bb D}}\def\EE{{\bb E}}
  \def\GG{{\bb G}}\def\HH{{\bb H}}\def\KK{{\bb K}}\def\LL{{\bb L}}
  \def\MM{{\bb M}}\def\QQ{{\bb Q}}\def\TT{{\bb T}}\def\YY{{\bb Y}}
  \def\PP{{\bb P}}\def\II{{\bb I}}\def\WW{{\bb W}}\def\XX{{\bb X}}
  \def\VV{{\bb V}}\def\SS{{\bb S}}\def\BB{{\bb B}}\def\NN{{\bb N}}
  \def\RR{{\bb R}}\def\ZZ{{\bb Z}}\def\FF{{\bb F}}\def\DD{{\bb D}}
  \def\OO{{\bb O}}\def\JJ{{\bb J}}\def\UU{{\bb U}}

\def\cH{\mathcal H}
\def\cY{\mathcal Y}
\def\cX{\mathcal X}
\def\cA{\mathcal A}
\def\mC{\mathcal C}
\def\Ex{\mathbf E}

\def\bb{\mathbb}
%end of header
\def\hat{\widehat}
\def\bfX{\mathbf X}
\def\bfB{\mathbf B}
\def\bfA{\mathbf A}
\def\bSigma{\boldsymbol\Sigma}
\def\bsigma{\boldsymbol\sigma}

\def\bOmega{\boldsymbol\Omega}
\def\bmu{\boldsymbol\mu}
\def\bnu{\boldsymbol\nu}
\def\btau{\boldsymbol\tau}
\def\bTau{\boldsymbol{\mathcal T}}
\def\btheta{{\boldsymbol\theta}}
\def\bTheta{{\boldsymbol\Theta}}
\def\bPi{\boldsymbol\Pi}
\def\bX{\boldsymbol X\!}
\def\bx{\boldsymbol x}
\def\bS{\boldsymbol S}
\def\bbeta{\boldsymbol \beta}
\def\bY{\boldsymbol Y}
\def\bxi{\boldsymbol \xi}
\def\bpi{\boldsymbol \pi}
\def\ci{\perp\!\!\!\perp}
\def\bZ{\boldsymbol Z}
\def\11{\mathds 1}
\def\b1{\mathbf 1}
\def\Pb{\mathbf P}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand*{\pd}[3][]{\ensuremath{\frac{d^{#1} #2}{d#3^{#1}}}}


\parindent=0pt
\parskip=5pt
\renewcommand{\baselinestretch}{1.08}


\newtheorem{rem}{Remark}
\newtheorem{defi}{Definition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{pro}{Proposition}
\newtheorem{theo}{Theorem}
\newtheorem{fact}{Fact}
\newtheorem{hyp}{Hypothesis} 

% Number lines
\usepackage{lineno,xcolor}
\linenumbers
\setlength\linenumbersep{5pt}
\renewcommand\linenumberfont{\normalfont\tiny\sffamily\color{gray}}

\begin{document}

Let consider a general mixture model. We observe $N$ random variables
$x_1, x_2,...,x_N$ which are independently and identically distributed with $x_i\sim f_{\bpi}(x_i)$ where $f_{\bpi}$ is given by:
\begin{equation}
f_{\bpi}(x)=\sum_{j=1}^K\bpi_jf_j(x)
\end{equation}
Let suppose that each component density $f_i$ is known, but not necessarily Gaussian. We will focus on the estimation of the weights vector $\bpi\in\RR^K$ and assume that this vector is sparse. We will focus our study on the performance of the Maximum Likelihood Estimator (et excess risk ?). From a rewriting of the loglikelihood we can define $\Phi_N(\bpi)$ as following:
\begin{equation}
  \Phi_N(\bpi) = -\frac{1}{N}\sum_{i=1}^N\log f_{\bpi}(x_i)
\end{equation}
We can rewrite the minimization problem:
\begin{equation}
\hat\bpi \in \argmin_{\bpi\in\Pi}\big\{\Phi_N(\bpi)\big\},\quad \Pi = \big\{\bpi\in [0,1]^K: \sum_{j=1}^K\pi_j=1\big\}
\end{equation}
For theoretical objectives, we will make the following asumption:
\begin{hyp}
\label{hyp_obs_likely}
All realizations are not probably unlikely to be observed. Therefore $\exists m>0 \text{ such that } f_{\bpi}(x)\geq m$ for all $x\in\{x_1,\dots,x_N\}$.
\end{hyp}
Let denote $\displaystyle M= \max_{x\in \{x_1,\dots,x_N\}, j\in[K]}\{f_j(x)\}$, since $\bpi^T\b1= 1$ then $f_{\bpi}\leq M$. Therefore, $\forall \bpi \in \Pi, f_{\bpi}({x_1,\dots,x_N}) \in [m,M]$. We have the following lemma:
\begin{lem}
Under hypothesis \ref{hyp_obs_likely}, $\Phi_N$ is Lipschitz-smooth and strongly convex.
\begin{proof}
For each $ i\in[K], g_{x_i}(\bpi)= f_{\bpi}(x_i)$ is a linear function defined on the convex compact set $\Pi$ and it's image is the interval $[m,M]$ where $m>0$.  We will prove that $-\log$ is strongly convex on $[m,M]$.
\begin{equation}
\forall x \in [m,M], \frac{1}{M^2}\leq\pd[2]{(-\log)}{x}(x)= \frac{1}{x^2}\leq \frac{1}{m^2}.
\end{equation}
The first inequality proves the $1/M^2$-strong convexity of $-\log$, the second proves that it is $1/m^2$-Lipschitz smooth. The sum of strongly convex functions is strongly convex. Therefore, $\Phi_N$ is strongly convex.
\end{proof}
\end{lem}
With these nice property under assumption \ref{hyp_obs_likely}, the minimization problem can be rewritten as follows:
\begin{equation}
\label{min_prob}
\hat\bpi \in \argmin_{\bpi\in\Pi}\big\{\Phi_N(\bpi)\big\},\quad 
\Pi = \big\{\bpi\in [0,1]^K: \bpi^T\b1=1, 
\forall i\in [N],\sum_{j=1}^K\pi_j f_j(x_i)\geq m
\big\}
\end{equation}
In this work, we will study different loss function: $||\hat\bpi-\bpi^*||_1$, $||\hat\bpi-\bpi^*||_2$ and some $dist(f_{\hat\bpi},f_{\bpi^*})$ (donner un example). It turns out that this problem is close to the regression with random design in the context of transductive learning \cite{bellec2016} since we do not observe the true cluster labels in our problem. We can consider $\Phi_N$ as a function of two random variable $X_i$ and $\bpi$:
\begin{equation}
  \Phi_N(\bpi) = \frac{1}{N}\sum_{i=1}^N\varphi(x_i,\bpi)
\end{equation}
In this setting $\varphi(.,.)$ (in our problem it is $-log(f_.(.))$) is strongly convex and Lipschitz smooth. We will recall some interesting results for our work on regression with random design.\\

Let consider the following trace regression model :
\begin{equation}
Y_i = tr(X_i^T\bfB^*) + \xi_i \quad i=1,\dots,N
\end{equation}
with $B^*\in\RR^{pxq}$ and let assume that $rank(B^*)$ is small. Let denote $\bsigma=[\sigma_1,\dots,\sigma_p]$ the singular values of $B^*$. The rank of this matrix is given by $||\sigma||_0$. Unfortunately, the $L_0$ norm is not convex, we tackle this problem by considering the convex $L_1$ norm $||\bsigma||_1$. Assume the constraint $\bsigma^T\b1=1$, then according to \cite{Koltchinskii2016} (quel theoreme ?) an empirical risk minimization method or a Maximum Likelihood Estimator with this constraint leads to a sparse estimator $\hat \bfB$.\\

Therefore, it might be interesting to compare this result with our problem \ref{min_prob}

\bibliographystyle{plain}
\bibliography{ref}

\end{document}








